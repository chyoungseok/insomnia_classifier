{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding = utf-8 -*\n",
    "# @Timeï¼š  10:07\n",
    "# @File: MAF_CNN.py\n",
    "# @Software: PyCharm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel=32, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"SE input (x) by concat: ([32, 3], [32, 2], [32, 3]): \", x.shape)\n",
    "        b, c, _ = x.size()\n",
    "        print(\"b, c: {}, {}\".format(b, c))\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        print(\"after squeeze (squeeze the last dimension):\", y.shape)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        print(\"after exication (y): \", y.shape)\n",
    "        print(\"y.expand_as(x): \", y.expand_as(x).shape)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class MSA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSA, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(64, 32, (5,), (1,), dilation=(2,)),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 32, (4,), (1,), dilation=(3,)),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 32, (3,), (1,), dilation=(4,)),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.se = SELayer()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        # print(x1.shape)\n",
    "        x2 = self.conv2(x)\n",
    "        # print(x2.shape)\n",
    "        x3 = self.conv3(x)\n",
    "        # print(x3.shape)\n",
    "        out = torch.cat([x1, x2, x3], dim=2)\n",
    "        # print(out.shape)\n",
    "        out = self.se(out)\n",
    "        # print(out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MAF_CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MAF_CNN, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, (256,), (32,)),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool1d(8, 8),\n",
    "\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Conv1d(64, 128, (7,), (1,)),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 64, (7,), (1,)),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool1d(4, 4),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.msa = MSA()\n",
    "        self.ft = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x1):\n",
    "        x1 = self.cnn1(x1)\n",
    "        print(\"after CNN: \", x1.shape)\n",
    "\n",
    "        x1 = self.msa(x1)\n",
    "        print(\"after MSA (x * y.expand_as(x)): \", x1.shape)\n",
    "\n",
    "        x_concat = x1\n",
    "        # print(x_concat.shape)\n",
    "        x = self.dropout(x_concat)\n",
    "        x = self.ft(x)\n",
    "\n",
    "        out = self.fc(x)\n",
    "        x = self.out(out)\n",
    "        return out, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after CNN:  torch.Size([2, 64, 11])\n",
      "SE input (x) by concat: ([32, 3], [32, 2], [32, 3]):  torch.Size([2, 32, 8])\n",
      "b, c: 2, 32\n",
      "after squeeze (squeeze the last dimension): torch.Size([2, 32])\n",
      "after exication (y):  torch.Size([2, 32, 1])\n",
      "y.expand_as(x):  torch.Size([2, 32, 8])\n",
      "after MSA (x * y.expand_as(x)):  torch.Size([2, 32, 8])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 64, 473]          16,448\n",
      "       BatchNorm1d-2              [-1, 64, 473]             128\n",
      "              ReLU-3              [-1, 64, 473]               0\n",
      "         MaxPool1d-4               [-1, 64, 59]               0\n",
      "           Dropout-5               [-1, 64, 59]               0\n",
      "            Conv1d-6              [-1, 128, 53]          57,472\n",
      "       BatchNorm1d-7              [-1, 128, 53]             256\n",
      "              ReLU-8              [-1, 128, 53]               0\n",
      "            Conv1d-9               [-1, 64, 47]          57,408\n",
      "      BatchNorm1d-10               [-1, 64, 47]             128\n",
      "             ReLU-11               [-1, 64, 47]               0\n",
      "        MaxPool1d-12               [-1, 64, 11]               0\n",
      "           Conv1d-13                [-1, 32, 3]          10,272\n",
      "      BatchNorm1d-14                [-1, 32, 3]              64\n",
      "             ReLU-15                [-1, 32, 3]               0\n",
      "           Conv1d-16                [-1, 32, 2]           8,224\n",
      "      BatchNorm1d-17                [-1, 32, 2]              64\n",
      "             ReLU-18                [-1, 32, 2]               0\n",
      "           Conv1d-19                [-1, 32, 3]           6,176\n",
      "      BatchNorm1d-20                [-1, 32, 3]              64\n",
      "             ReLU-21                [-1, 32, 3]               0\n",
      "AdaptiveAvgPool1d-22                [-1, 32, 1]               0\n",
      "           Linear-23                    [-1, 2]              64\n",
      "             ReLU-24                    [-1, 2]               0\n",
      "           Linear-25                   [-1, 32]              64\n",
      "          Sigmoid-26                   [-1, 32]               0\n",
      "          SELayer-27                [-1, 32, 8]               0\n",
      "              MSA-28                [-1, 32, 8]               0\n",
      "          Dropout-29                [-1, 32, 8]               0\n",
      "          Flatten-30                  [-1, 256]               0\n",
      "           Linear-31                  [-1, 512]         131,584\n",
      "             ReLU-32                  [-1, 512]               0\n",
      "           Linear-33                  [-1, 128]          65,664\n",
      "             ReLU-34                  [-1, 128]               0\n",
      "           Linear-35                    [-1, 2]             258\n",
      "================================================================\n",
      "Total params: 354,338\n",
      "Trainable params: 354,338\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 1.00\n",
      "Params size (MB): 1.35\n",
      "Estimated Total Size (MB): 2.41\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from my_model import SimpleSleepPPGModel\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "net = MAF_CNN(2).to(device)\n",
    "model = net.cuda()\n",
    "summary(model, (1, 15360))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ins_classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
